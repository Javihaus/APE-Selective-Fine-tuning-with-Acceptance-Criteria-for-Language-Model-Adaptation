{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuxU3rt491cF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries for environment checks\n",
        "import sys\n",
        "import pkg_resources\n",
        "import platform\n",
        "import datetime\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress FutureWarning for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Log notebook execution timestamp\n",
        "print(f\"Notebook executed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "metadata": {
        "id": "4G1txkGdHWYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define required packages and minimum versions\n",
        "REQUIRED_PACKAGES = {\n",
        "    \"transformers\": \"4.28.0\",\n",
        "    \"datasets\": \"2.14.0\",\n",
        "    \"torch\": \"2.0.0\",\n",
        "    \"nltk\": \"3.8.0\",\n",
        "    \"rouge_score\": \"0.1.0\",\n",
        "    \"bert_score\": \"0.3.0\",\n",
        "    \"matplotlib\": \"3.7.0\",\n",
        "    \"seaborn\": \"0.12.0\",\n",
        "    \"pandas\": \"2.0.0\",\n",
        "    \"numpy\": \"1.23.0\"\n",
        "}\n",
        "\n",
        "def check_environment() -> bool:\n",
        "    \"\"\"Verify the Python environment meets minimum requirements.\"\"\"\n",
        "    print(\"\\n=== Environment Verification ===\")\n",
        "\n",
        "    # Check Python version\n",
        "    python_version = sys.version_info\n",
        "    if python_version < (3, 8):\n",
        "        print(f\"Error: Python {python_version.major}.{python_version.minor} detected. Requires 3.8+.\")\n",
        "        return False\n",
        "    print(f\"Python Version: {platform.python_version()} - OK\")\n",
        "\n",
        "    # Check package versions\n",
        "    for pkg, min_version in REQUIRED_PACKAGES.items():\n",
        "        try:\n",
        "            installed_version = pkg_resources.get_distribution(pkg).version\n",
        "            if pkg_resources.parse_version(installed_version) < pkg_resources.parse_version(min_version):\n",
        "                print(f\"Error: {pkg} version {installed_version} is below required {min_version}.\")\n",
        "                return False\n",
        "            print(f\"{pkg}: {installed_version} - OK\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"Error: {pkg} is not installed. Install with `pip install {pkg}`.\")\n",
        "            return False\n",
        "\n",
        "    # Check GPU availability\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Warning: No GPU detected. CPU will be used, but T4 GPU is recommended.\")\n",
        "    else:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GiB) - OK\")\n",
        "\n",
        "    # Check Google Drive connectivity\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive: Mounted successfully - OK\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Google Drive mount failed - {str(e)}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Run environment check\n",
        "if not check_environment():\n",
        "    raise SystemExit(\"Environment check failed. Please resolve issues and rerun.\")\n"
      ],
      "metadata": {
        "id": "2L53BdLBHcMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Install dependencies if not pre-installed\n",
        "# !pip install transformers datasets torch nltk rouge_score bert-score matplotlib seaborn pandas numpy"
      ],
      "metadata": {
        "id": "IHbyDrzGG_gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "from nltk.translate import bleu_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "import nltk\n",
        "\n",
        "# Set random seeds for reproducibility across libraries\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)  # For multi-GPU setups\n",
        "\n",
        "# Suppress FutureWarning for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "id": "ptwyUaGR92iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Notebook configuration"
      ],
      "metadata": {
        "id": "YCksSQT-DMOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log notebook execution timestamp\n",
        "print(f\"Notebook executed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "metadata": {
        "id": "wL1i4UWkGAvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "# Usage note\n",
        "print(\"\"\"\n",
        "=== Usage Note ===\n",
        "Run all cells sequentially. Expected runtime is ~2-3 hours on a T4 GPU.\n",
        "Outputs (models, results) are saved to Google Drive at {BASE_DIR}.\n",
        "Ensure sufficient storage (~5 GB) and a stable connection.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "ZsDGrQYuCTIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define base directory in Google Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/NeurIPS2025_Results\"\n",
        "for subdir in [\"models\", \"plots\"]:\n",
        "    os.makedirs(os.path.join(BASE_DIR, subdir), exist_ok=True)"
      ],
      "metadata": {
        "id": "vMWCrProDKkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define base directory in Google Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/NeurIPS2025_Results\"\n",
        "for subdir in [\"models\", \"qualitative_analysis\"]:\n",
        "    os.makedirs(os.path.join(BASE_DIR, subdir), exist_ok=True)\n",
        "\n",
        "# Function to print GPU memory usage\n",
        "def print_gpu_memory() -> None:\n",
        "    \"\"\"Print current GPU memory usage in GiB.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        print(f\"GPU Memory - Allocated: {allocated:.2f} GiB, Reserved: {reserved:.2f} GiB\")"
      ],
      "metadata": {
        "id": "oiCPL0VDDHeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8-paper')\n",
        "sns.set_context(\"paper\")\n",
        "plt.rcParams.update({\n",
        "    'font.family': 'serif',\n",
        "    'font.serif': ['DejaVu Serif'],\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 14,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.figsize': (7, 5),\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'savefig.format': 'pdf',\n",
        "    'savefig.bbox': 'tight',\n",
        "    'axes.grid': True,\n",
        "    'grid.linestyle': '--',\n",
        "    'grid.alpha': 0.7\n",
        "})"
      ],
      "metadata": {
        "id": "tEoH1S0dzoDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model and tokenizer"
      ],
      "metadata": {
        "id": "exngDgSiDi2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5-base in FP32 with mixed precision\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-base\",\n",
        "    torch_dtype=torch.float32\n",
        "    ).to(device)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "articles = dataset['train']['article'][:4000]\n",
        "summaries = dataset['train']['highlights'][:4000]\n",
        "test_articles = dataset['test']['article'][:1000]\n",
        "test_summaries = dataset['test']['highlights'][:1000]\n"
      ],
      "metadata": {
        "id": "DnIGzCQO-DtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model functions"
      ],
      "metadata": {
        "id": "P9wefaTFDtSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_model(\n",
        "    model: T5ForConditionalGeneration,\n",
        "    articles: List[str],\n",
        "    summaries: List[str],\n",
        "    learning_rate: float,\n",
        "    epochs: int = 3,\n",
        "    accum_steps: int = 4\n",
        ") -> None:\n",
        "    \"\"\"Fine-tune the model on a batch of articles and summaries\n",
        "    using APE perturbation.\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    model.train()\n",
        "\n",
        "    print(\"Before fine-tuning:\")\n",
        "    print_gpu_memory()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        raw_loss = 0\n",
        "        for i, (article, summary) in enumerate(zip(articles, summaries)):\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            inputs = tokenizer(\n",
        "                article, return_tensors=\"pt\", max_length=512,\n",
        "                truncation=True, padding=True\n",
        "            ).to(device)\n",
        "            targets = tokenizer(\n",
        "                summary, return_tensors=\"pt\", max_length=128,\n",
        "                truncation=True, padding=True\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(\n",
        "                    input_ids=inputs.input_ids,\n",
        "                    attention_mask=inputs.attention_mask,\n",
        "                    labels=targets.input_ids\n",
        "                )\n",
        "                loss = outputs.loss / accum_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            raw_loss += loss.item()\n",
        "\n",
        "            if (i + 1) % accum_steps == 0 or (i + 1) == len(articles):\n",
        "                scaler.unscale_(optimizer)\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                print(f\"Epoch {epoch+1}, Step {i+1}: Loss = {raw_loss:.4f}, Grad Norm = {grad_norm:.4f}\")\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                raw_loss = 0\n",
        "\n",
        "    print(\"After fine-tuning:\")\n",
        "    print_gpu_memory()\n",
        "\n",
        "\n",
        "def generate_summaries(\n",
        "    model: T5ForConditionalGeneration,\n",
        "    articles: List[str]\n",
        "    ) -> List[str]:\n",
        "    \"\"\"Generate summaries for a list of articles.\"\"\"\n",
        "    model.eval()\n",
        "    summaries = []\n",
        "    for article in articles:\n",
        "        inputs = tokenizer(\n",
        "            article, return_tensors=\"pt\", max_length=512,\n",
        "            truncation=True, padding=True\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model.generate(\n",
        "                    input_ids=inputs.input_ids,\n",
        "                    attention_mask=inputs.attention_mask,\n",
        "                    max_length=128\n",
        "                )\n",
        "        summaries.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    return summaries\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    generated_summaries: List[str],\n",
        "    reference_summaries: List[str]\n",
        ") -> Tuple[Dict[str, float], Dict[str, float]]:\n",
        "\n",
        "    \"\"\"Compute BLEU, ROUGE-1, BERTScore, and perplexity for generated summaries.\"\"\"\n",
        "    bleu_scores = [\n",
        "        bleu_score.sentence_bleu([ref.split()], gen.split())\n",
        "        for ref, gen in zip(reference_summaries, generated_summaries)\n",
        "    ]\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    rouge1_scores = [\n",
        "        scorer.score(ref, gen)['rouge1'].fmeasure\n",
        "        for ref, gen in zip(reference_summaries, generated_summaries)\n",
        "    ]\n",
        "    _, _, bert_scores = bert_score(generated_summaries, reference_summaries, lang=\"en\")\n",
        "    bert_scores = bert_scores.tolist()\n",
        "\n",
        "    # Compute perplexity (average cross-entropy loss)\n",
        "    model.eval()\n",
        "    perplexities = []\n",
        "    for article, summary in zip(test_articles, generated_summaries):\n",
        "        inputs = tokenizer(\n",
        "            article, return_tensors=\"pt\", max_length=512,\n",
        "            truncation=True, padding=True\n",
        "        ).to(device)\n",
        "        targets = tokenizer(\n",
        "            summary, return_tensors=\"pt\", max_length=128,\n",
        "            truncation=True, padding=True\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(\n",
        "                    input_ids=inputs.input_ids,\n",
        "                    attention_mask=inputs.attention_mask,\n",
        "                    labels=targets.input_ids\n",
        "                )\n",
        "        perplexities.append(torch.exp(outputs.loss).item())\n",
        "\n",
        "    means = {\n",
        "        \"bleu\": np.mean(bleu_scores),\n",
        "        \"rouge1\": np.mean(rouge1_scores),\n",
        "        \"bertscore\": np.mean(bert_scores),\n",
        "        \"perplexity\": np.mean(perplexities)\n",
        "    }\n",
        "    stds = {\n",
        "        \"bleu\": np.std(bleu_scores),\n",
        "        \"rouge1\": np.std(rouge1_scores),\n",
        "        \"bertscore\": np.std(bert_scores),\n",
        "        \"perplexity\": np.std(perplexities)\n",
        "    }\n",
        "    return means, stds\n",
        "\n",
        "def plot_metric(\n",
        "    iterations: List[int],\n",
        "    means: List[float],\n",
        "    stds: List[float],\n",
        "    title: str,\n",
        "    ylabel: str,\n",
        "    color: str,\n",
        "    filename: str\n",
        ") -> plt.Figure:\n",
        "\n",
        "    \"\"\"Create a publication-quality plot for a given metric.\"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.errorbar(\n",
        "        iterations, means, yerr=stds, fmt='-o', color=color, ecolor='gray',\n",
        "        capsize=4, capthick=1, linewidth=2, markersize=6, label='APE Iterations'\n",
        "    )\n",
        "    ax.axhline(y=means[0], color='red', linestyle='--', linewidth=2, label='Baseline')\n",
        "    ax.fill_between(iterations, [m - s for m, s in zip(means, stds)],\n",
        "                    [m + s for m, s in zip(means, stds)], alpha=0.2, color=color)\n",
        "    z = np.polyfit(iterations, means, 3)\n",
        "    p = np.poly1d(z)\n",
        "    x_trend = np.linspace(min(iterations), max(iterations), 100)\n",
        "    ax.plot(x_trend, p(x_trend), '--', color='darkgray', alpha=0.5)\n",
        "    ax.set_xlabel('Iteration', fontweight='bold')\n",
        "    ax.set_ylabel(ylabel, fontweight='bold')\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xticks(iterations)\n",
        "    ax.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(BASE_DIR, \"plots\", filename))\n",
        "    plt.close()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "g2AHpgwx_rna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Run experiment"
      ],
      "metadata": {
        "id": "X20WEB17EPr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save baseline model\n",
        "model.save_pretrained(os.path.join(BASE_DIR, \"models/baseline_model\"))\n",
        "tokenizer.save_pretrained(os.path.join(BASE_DIR, \"models/baseline_model\"))\n",
        "print(\"Baseline model saved.\")\n",
        "\n",
        "# Generate baseline summaries and compute metrics\n",
        "baseline_summaries = generate_summaries(model, test_articles)\n",
        "baseline_means, baseline_stds = compute_metrics(baseline_summaries, test_summaries)\n",
        "print(\"Baseline metrics computed.\")\n",
        "\n",
        "# Run APE perturbations (17 iterations, ~235 articles per batch)\n",
        "fixed_lr = 3e-6\n",
        "batch_size = 235  # 4,000 articles / 17 iterations ≈ 235\n",
        "perturbations = [\n",
        "    (train_articles[i:i + batch_size], train_summaries[i:i + batch_size], fixed_lr)\n",
        "    for i in range(0, len(train_articles), batch_size)\n",
        "]\n",
        "\n",
        "# Store metrics across iterations (0 = baseline)\n",
        "metrics_history = {\n",
        "    \"bleu_means\": [baseline_means[\"bleu\"]],\n",
        "    \"bleu_stds\": [baseline_stds[\"bleu\"]],\n",
        "    \"rouge1_means\": [baseline_means[\"rouge1\"]],\n",
        "    \"rouge1_stds\": [baseline_stds[\"rouge1\"]],\n",
        "    \"bertscore_means\": [baseline_means[\"bertscore\"]],\n",
        "    \"bertscore_stds\": [baseline_stds[\"bertscore\"]],\n",
        "    \"perplexity_means\": [baseline_means[\"perplexity\"]],\n",
        "    \"perplexity_stds\": [baseline_stds[\"perplexity\"]]\n",
        "}\n",
        "\n",
        "for i, (articles, summaries, lr) in enumerate(perturbations, 1):\n",
        "    print(f\"Iteration {i}: Perturbing with lr={lr}, {len(articles)} articles\")\n",
        "    perturb_model(model, articles, summaries, lr)\n",
        "    summaries = generate_summaries(model, test_articles)\n",
        "    means, stds = compute_metrics(summaries, test_summaries)\n",
        "    metrics_history[\"bleu_means\"].append(means[\"bleu\"])\n",
        "    metrics_history[\"bleu_stds\"].append(stds[\"bleu\"])\n",
        "    metrics_history[\"rouge1_means\"].append(means[\"rouge1\"])\n",
        "    metrics_history[\"rouge1_stds\"].append(stds[\"rouge1\"])\n",
        "    metrics_history[\"bertscore_means\"].append(means[\"bertscore\"])\n",
        "    metrics_history[\"bertscore_stds\"].append(stds[\"bertscore\"])\n",
        "    metrics_history[\"perplexity_means\"].append(means[\"perplexity\"])\n",
        "    metrics_history[\"perplexity_stds\"].append(stds[\"perplexity\"])\n",
        "\n",
        "# Save final model\n",
        "model.save_pretrained(os.path.join(BASE_DIR, \"models/final_model\"))\n",
        "tokenizer.save_pretrained(os.path.join(BASE_DIR, \"models/final_model\"))\n",
        "print(\"Final model saved.\")\n"
      ],
      "metadata": {
        "id": "kff8rd-BENX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Results visualization"
      ],
      "metadata": {
        "id": "7c89eVvHEYe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "iterations = list(range(18))  # 0 (baseline) + 17 iterations\n",
        "plot_metric(\n",
        "    iterations, metrics_history[\"bleu_means\"], metrics_history[\"bleu_stds\"],\n",
        "    \"BLEU Score Across APE Iterations (T5-base, Fixed LR)\",\n",
        "    \"BLEU Score\", \"blue\", \"bleu_plot_t5base_fixed_lr.pdf\"\n",
        ")\n",
        "plot_metric(\n",
        "    iterations, metrics_history[\"rouge1_means\"], metrics_history[\"rouge1_stds\"],\n",
        "    \"ROUGE-1 Score Across APE Iterations (T5-base, Fixed LR)\",\n",
        "    \"ROUGE-1 Score\", \"darkorange\", \"rouge1_plot_t5base_fixed_lr.pdf\"\n",
        ")\n",
        "plot_metric(\n",
        "    iterations, metrics_history[\"bertscore_means\"], metrics_history[\"bertscore_stds\"],\n",
        "    \"BERTScore Across APE Iterations (T5-base, Fixed LR)\",\n",
        "    \"BERTScore F1\", \"purple\", \"bertscore_plot_t5base_fixed_lr.pdf\"\n",
        ")\n",
        "plot_metric(\n",
        "    iterations, metrics_history[\"perplexity_means\"], metrics_history[\"perplexity_stds\"],\n",
        "    \"Perplexity Across APE Iterations (T5-base, Fixed LR)\",\n",
        "    \"Perplexity\", \"green\", \"perplexity_plot_t5base_fixed_lr.pdf\"\n",
        ")\n",
        "print(\"Plots saved to Google Drive.\")\n",
        "\n",
        "# Save metrics history\n",
        "with open(os.path.join(BASE_DIR, \"results_summary.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(metrics_history, f)\n",
        "print(\"Metrics history saved.\")\n"
      ],
      "metadata": {
        "id": "a95_C850Ev_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flush and unmount Google Drive\n",
        "drive.flush_and_unmount()\n",
        "print(f\"All results saved to {BASE_DIR}\")\n"
      ],
      "metadata": {
        "id": "P8N4J7tJE0uN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
